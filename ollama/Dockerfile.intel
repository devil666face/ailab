# https://github.com/mattcurf/ollama-intel-gpu
# https://github.com/eleiton/ollama-intel-arc/blob/main/docker-compose.yml

FROM intelanalytics/ipex-llm-inference-cpp-xpu:latest

ENV DEBIAN_FRONTEND=noninteractive \
    LANG=en_US.UTF-8 \
    LANGUAGE=en_US:en \
    LC_ALL=en_US.UTF-8 \
    TZ=UTC \
    PATH="/llm:/root/.local/bin:/root/.venv/bin:${PATH}"

RUN init-ollama

WORKDIR /root

RUN apt-get update --quiet --quiet && \
    apt-get install --quiet --quiet --yes \
    curl \
    && apt-get --quiet --quiet clean \
    && rm --recursive --force /var/lib/apt/lists/* /tmp/* /var/tmp/*

RUN curl -LsSf https://astral.sh/uv/install.sh | sh

COPY pyproject.toml .
COPY uv.lock .
COPY .python-version .

RUN uv sync
# RUN uv sync --extra unsloth

COPY entrypoint.sh /entrypoint.sh

ENTRYPOINT ["/entrypoint.sh"]
